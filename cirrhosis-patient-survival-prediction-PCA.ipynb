{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder, PowerTransformer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold, cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "original = pd.read_csv('original.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "ss = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process(train, test, original):\n",
    "    df_train = train.drop(['id', 'Status'], axis=1)\n",
    "    original = original.dropna()\n",
    "    df_original = original.drop(['ID', 'Status'], axis=1)\n",
    "    df_train = pd.concat([df_train, df_original])\n",
    "    df_test = test.drop(['id'], axis=1)\n",
    "    \n",
    "    # Categorical Cols\n",
    "    # Train\n",
    "    categorical_cols = ['Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Stage']\n",
    "    encoder = OneHotEncoder(drop='first')\n",
    "    encoder.fit(df_train[categorical_cols])\n",
    "    df_train_cat = pd.DataFrame(encoder.transform(df_train[categorical_cols]).toarray(), columns=encoder.get_feature_names_out())\n",
    "    # Test\n",
    "    df_test_cat = pd.DataFrame(encoder.transform(df_test[categorical_cols]).toarray(), columns=encoder.get_feature_names_out())\n",
    "    \n",
    "    # Numerical Cols\n",
    "    # Train\n",
    "    df_train_num = df_train.drop(categorical_cols, axis=1)\n",
    "    df_train_num = np.log1p(df_train_num)\n",
    "    scaler = StandardScaler()\n",
    "    df_train_num = pd.DataFrame(scaler.fit_transform(df_train_num), columns=df_train_num.columns)\n",
    "    p_transformer = PowerTransformer()\n",
    "    df_train_num = pd.DataFrame(p_transformer.fit_transform(df_train_num), columns=df_train_num.columns)\n",
    "    # Test\n",
    "    df_test_num = df_test.drop(categorical_cols, axis=1)\n",
    "    df_test_num = np.log1p(df_test_num)\n",
    "    df_test_num = pd.DataFrame(scaler.transform(df_test_num), columns=df_test_num.columns)\n",
    "    df_test_num = pd.DataFrame(p_transformer.transform(df_test_num), columns=df_test_num.columns)\n",
    "    \n",
    "    # Combine Num/Cat\n",
    "    train_final = pd.concat([df_train_num, df_train_cat], axis=1)\n",
    "    test_final = pd.concat([df_test_num, df_test_cat], axis=1)\n",
    "    \n",
    "    # PCA\n",
    "    pca = PCA(n_components=3)\n",
    "    pca_train = pca.fit_transform(train_final)\n",
    "    pca_features_train = pd.DataFrame(pca_train, columns=['PCA1', 'PCA2', 'PCA3'])\n",
    "    train_final = pd.concat([train_final, pca_features_train], axis=1)\n",
    "    \n",
    "    pca_test = pca.transform(test_final)\n",
    "    pca_features_test = pd.DataFrame(pca_test, columns=['PCA1', 'PCA2', 'PCA3'])\n",
    "    test_final = pd.concat([test_final, pca_features_test], axis=1)\n",
    "    \n",
    "    # Feature Engineering\n",
    "    # https://www.kaggle.com/code/ashishkumarak/ps3e26-liver-cirrhosis-survival-prediction#%F0%9F%92%BB-Feature-Engineering\n",
    "    threshold_platelets = 150\n",
    "    train_final['thrombocytopenia'] = np.where(train_final['Platelets'] < threshold_platelets, 1, 0)\n",
    "    test_final['thrombocytopenia'] = np.where(test_final['Platelets'] < threshold_platelets, 1, 0)\n",
    "    threshold_alk_phos_upper = 147  # Upper limit of normal range\n",
    "    threshold_alk_phos_lower = 44   # Lower limit of normal range\n",
    "    train_final['elevated_alk_phos'] = np.where((train_final['Alk_Phos'] > threshold_alk_phos_upper) | (train_final['Alk_Phos'] < threshold_alk_phos_lower), 1, 0)\n",
    "    test_final['elevated_alk_phos'] = np.where((test_final['Alk_Phos'] > threshold_alk_phos_upper) | (test_final['Alk_Phos'] < threshold_alk_phos_lower), 1, 0)\n",
    "    normal_copper_range = (62, 140)\n",
    "    train_final['normal_copper'] = np.where((train_final['Copper'] >= normal_copper_range[0]) & (train_final['Copper'] <= normal_copper_range[1]), 1, 0)\n",
    "    test_final['normal_copper'] = np.where((test_final['Copper'] >= normal_copper_range[0]) & (test_final['Copper'] <= normal_copper_range[1]), 1, 0)\n",
    "    normal_albumin_range = (3.4, 5.4)\n",
    "    train_final['normal_albumin'] = np.where((train_final['Albumin'] >= normal_albumin_range[0]) & (train_final['Albumin'] <= normal_albumin_range[1]), 1, 0)\n",
    "    test_final['normal_albumin'] = np.where((test_final['Albumin'] >= normal_albumin_range[0]) & (test_final['Albumin'] <= normal_albumin_range[1]), 1, 0)\n",
    "    normal_bilirubin_range = (0.2, 1.2)\n",
    "    train_final['normal_bilirubin'] = np.where((train_final['Bilirubin'] >= normal_bilirubin_range[0]) & (train_final['Bilirubin'] <= normal_bilirubin_range[1]), 1, 0)\n",
    "    test_final['normal_bilirubin'] = np.where((test_final['Bilirubin'] >= normal_bilirubin_range[0]) & (test_final['Bilirubin'] <= normal_bilirubin_range[1]), 1, 0)\n",
    "    \n",
    "    # Encode Target\n",
    "    le_encoder = LabelEncoder()\n",
    "    y = le_encoder.fit_transform(train.Status)\n",
    "    y_original = le_encoder.fit_transform(original.Status)\n",
    "    \n",
    "    # Split X, y\n",
    "    X_train = train_final\n",
    "    y_train = np.concatenate([y, y_original])\n",
    "    \n",
    "    X_test = test_final\n",
    "    \n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = train.drop(['id', 'Status'], axis=1)\n",
    "# original = original.dropna()\n",
    "# df_original = original.drop(['ID', 'Status'], axis=1)\n",
    "# X = pd.concat([df_train, df_original])\n",
    "# y = pd.concat([train.Status, original.Status])\n",
    "# categorical_cols = ['Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Stage']\n",
    "\n",
    "# class MyTransformer(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(self):\n",
    "#         self.encoder = OneHotEncoder(drop='first')\n",
    "#         self.scaler = StandardScaler()\n",
    "#         self.p_transformer = PowerTransformer()\n",
    "#         self.le_encoder = LabelEncoder()\n",
    "        \n",
    "#     def fit(self, X, y):\n",
    "#         X_cat = pd.DataFrame(self.encoder.fit_transform(X[categorical_cols]).toarray(), columns=self.encoder.get_feature_names_out())\n",
    "#         X_num = X.drop(categorical_cols, axis=1)\n",
    "#         try:\n",
    "#             X_num = np.log1p(X_num)\n",
    "#         except Exception as E:\n",
    "#             print(E)\n",
    "#         X_num = pd.DataFrame(self.scaler.fit_transform(X_num), columns=X_num.columns)\n",
    "#         X_num = pd.DataFrame(self.p_transformer.fit_transform(X_num), columns=X_num.columns)\n",
    "#         X = pd.concat([X_num, X_cat], axis=1)\n",
    "        \n",
    "#         # https://www.kaggle.com/code/ashishkumarak/ps3e26-liver-cirrhosis-survival-prediction#%F0%9F%92%BB-Feature-Engineering\n",
    "#         threshold_platelets = 150\n",
    "#         X['thrombocytopenia'] = np.where(X['Platelets'] < threshold_platelets, 1, 0)\n",
    "#         threshold_alk_phos_upper = 147  # Upper limit of normal range\n",
    "#         threshold_alk_phos_lower = 44   # Lower limit of normal range\n",
    "#         X['elevated_alk_phos'] = np.where((X['Alk_Phos'] > threshold_alk_phos_upper) | (X['Alk_Phos'] < threshold_alk_phos_lower), 1, 0)\n",
    "#         normal_copper_range = (62, 140)\n",
    "#         X['normal_copper'] = np.where((X['Copper'] >= normal_copper_range[0]) & (X['Copper'] <= normal_copper_range[1]), 1, 0)\n",
    "#         normal_albumin_range = (3.4, 5.4)\n",
    "#         X['normal_albumin'] = np.where((X['Albumin'] >= normal_albumin_range[0]) & (X['Albumin'] <= normal_albumin_range[1]), 1, 0)\n",
    "#         normal_bilirubin_range = (0.2, 1.2)\n",
    "#         X['normal_bilirubin'] = np.where((X['Bilirubin'] >= normal_bilirubin_range[0]) & (X['Bilirubin'] <= normal_bilirubin_range[1]), 1, 0)\n",
    "            \n",
    "#         y = self.le_encoder.fit_transform(y)\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         X_cat = pd.DataFrame(self.encoder.transform(X[categorical_cols]).toarray(), columns=self.encoder.get_feature_names_out())\n",
    "#         X_num = X.drop(categorical_cols, axis=1)\n",
    "#         X_num = np.log1p(X_num)\n",
    "#         X_num = pd.DataFrame(self.scaler.transform(X_num), columns=X_num.columns)\n",
    "#         X_num = pd.DataFrame(self.p_transformer.transform(X_num), columns=X_num.columns)\n",
    "#         X = pd.concat([X_num, X_cat], axis=1)\n",
    "        \n",
    "#         threshold_platelets = 150\n",
    "#         X['thrombocytopenia'] = np.where(X['Platelets'] < threshold_platelets, 1, 0)\n",
    "#         threshold_alk_phos_upper = 147  # Upper limit of normal range\n",
    "#         threshold_alk_phos_lower = 44   # Lower limit of normal range\n",
    "#         X['elevated_alk_phos'] = np.where((X['Alk_Phos'] > threshold_alk_phos_upper) | (X['Alk_Phos'] < threshold_alk_phos_lower), 1, 0)\n",
    "#         normal_copper_range = (62, 140)\n",
    "#         X['normal_copper'] = np.where((X['Copper'] >= normal_copper_range[0]) & (X['Copper'] <= normal_copper_range[1]), 1, 0)\n",
    "#         normal_albumin_range = (3.4, 5.4)\n",
    "#         X['normal_albumin'] = np.where((X['Albumin'] >= normal_albumin_range[0]) & (X['Albumin'] <= normal_albumin_range[1]), 1, 0)\n",
    "#         normal_bilirubin_range = (0.2, 1.2)\n",
    "#         X['normal_bilirubin'] = np.where((X['Bilirubin'] >= normal_bilirubin_range[0]) & (X['Bilirubin'] <= normal_bilirubin_range[1]), 1, 0)\n",
    "        \n",
    "#         return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = abs(cross_val_predict(make_pipeline(MyTransformer(), RandomForestClassifier()), X, y, cv=5, method='predict_proba'))\n",
    "# print(\"RF Log Loss:\", round(log_loss(y, scores), 4))\n",
    "\n",
    "# scores = abs(cross_val_predict(make_pipeline(MyTransformer(), XGBClassifier(verbosity=0, use_label_encoder=False)), X, y, cv=5, method='predict_proba'))\n",
    "# print(\"XGB Log Loss:\", round(log_loss(y, scores), 4))\n",
    "\n",
    "# scores = abs(cross_val_predict(make_pipeline(MyTransformer(), LGBMClassifier(verbose=-1)), X, y, cv=5, method='predict_proba'))\n",
    "# print(\"LGBM Log Loss:\", round(log_loss(y, scores), 4))\n",
    "\n",
    "# scores = abs(cross_val_predict(make_pipeline(MyTransformer(), CatBoostClassifier(verbose=0)), X, y, cv=5, method='predict_proba'))\n",
    "# print(\"CatBoost Log Loss:\", round(log_loss(y, scores), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, test_final = process(train, test, original)\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-10, 1, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-10, 1, log=True),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-10, 1, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'objective': 'mlogloss', \n",
    "        'eval_metric': 'logloss',\n",
    "        'verbosity': 0,\n",
    "        'use_label_encoder': False,\n",
    "        'random_state': 0,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    model = XGBClassifier(**params)\n",
    "\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    cv_scores = abs(cross_val_score(model, X, y, scoring='neg_log_loss', cv=cv))\n",
    "    \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "def lgbm(trial):\n",
    "    params = {\n",
    "        'objective': 'multiclass',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'metric': 'multi_logloss',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 50),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 100),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-9, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-9, 10.0, log=True),\n",
    "        'verbose': -1,\n",
    "        'random_state': 0\n",
    "    }\n",
    "\n",
    "    model = LGBMClassifier(**params)\n",
    "\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    cv_scores = abs(cross_val_score(model, X, y, scoring='neg_log_loss', cv=cv))\n",
    "    \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "def catboost(trial):\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'depth': trial.suggest_int('depth', 1, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.1, 10.0),\n",
    "        'random_seed': 0,\n",
    "        'loss_function': 'MultiClass',\n",
    "        'eval_metric': 'MultiClass',\n",
    "        'logging_level': 'Silent'\n",
    "    }\n",
    "    \n",
    "    model = CatBoostClassifier(**params)\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    \n",
    "    cv_scores = abs(cross_val_score(model, X, y, scoring='neg_log_loss', cv=cv))\n",
    "    \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# study = optuna.create_study(direction='minimize')\n",
    "# optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "# study.optimize(xgb, n_trials=100)\n",
    "# study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# study = optuna.create_study(direction='minimize')\n",
    "# optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "# study.optimize(lgbm, n_trials=100)\n",
    "# study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-12-26 16:27:52,968]\u001b[0m A new study created in memory with name: no-name-394d393b-902a-4804-8d54-28dad4f505e4\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'iterations': 950,\n",
       " 'depth': 3,\n",
       " 'learning_rate': 0.07759505269501321,\n",
       " 'l2_leaf_reg': 1.3999226082174185}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# study = optuna.create_study(direction='minimize')\n",
    "# optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "# study.optimize(catboost, n_trials=100)\n",
    "# study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'n_estimators': 807,\n",
    "    'max_depth': 9,\n",
    "    'learning_rate': 0.0188649127427861,\n",
    "    'subsample': 0.8274515628887853,\n",
    "    'colsample_bytree': 0.11623733296601546,\n",
    "    'reg_alpha': 0.0017239053749286994,\n",
    "    'reg_lambda': 0.001115134043916317,\n",
    "    'gamma': 6.23346663072776e-05,\n",
    "    'min_child_weight': 9,\n",
    "    \n",
    "    'objective': 'mlogloss', \n",
    "    'eval_metric': 'logloss',\n",
    "    'verbosity': 0,\n",
    "    'use_label_encoder': False\n",
    "}\n",
    "\n",
    "lgbm_params = {\n",
    "    'n_estimators': 422,\n",
    "    'max_depth': 27,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.017847511069651788,\n",
    "    'min_child_samples': 96,\n",
    "    'subsample': 0.26791725059736793,\n",
    "    'colsample_bytree': 0.37621860984436856,\n",
    "    'reg_alpha': 3.7054057635827013e-09,\n",
    "    'reg_lambda': 4.861640891517785e-08,\n",
    "    \n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "catboost_params = {\n",
    "    'iterations': 950,\n",
    "    'depth': 3,\n",
    "    'learning_rate': 0.07759505269501321,\n",
    "    'l2_leaf_reg': 1.3999226082174185,\n",
    "    \n",
    "    'loss_function': 'MultiClass',\n",
    "    'eval_metric': 'MultiClass',\n",
    "    'logging_level': 'Silent'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(**xgb_params)\n",
    "lgbm_model = LGBMClassifier(**lgbm_params)\n",
    "catboost_model = CatBoostClassifier(**catboost_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'xgb': xgb_model,\n",
    "    'ltgbm': lgbm_model,\n",
    "    'catboost': catboost_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb\n",
      "ltgbm\n",
      "catboost\n"
     ]
    }
   ],
   "source": [
    "results_ensemble_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(name)\n",
    "    res=[]\n",
    "    for i, (train_index, test_index) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict_proba(X_test)\n",
    "        res.append(log_loss(y_test, y_pred))\n",
    "    results_ensemble_models[name] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "xgb\n",
      "0.42128913385978456\n",
      "0.012098906142766559\n",
      "----------\n",
      "ltgbm\n",
      "0.43070148874977054\n",
      "0.011599518861663459\n",
      "----------\n",
      "catboost\n",
      "0.44118162297442554\n",
      "0.011323651482178992\n"
     ]
    }
   ],
   "source": [
    "for name, result in results_ensemble_models.items():\n",
    "    print(\"----------\\n\" + name)\n",
    "    print(np.mean(result))\n",
    "    print(np.std(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_model = VotingClassifier(estimators=[('xgb', xgb_model),\n",
    "                                           ('lgbm', lgbm_model)],\n",
    "                               voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss: 0.4199\n"
     ]
    }
   ],
   "source": [
    "results_ensemble = []\n",
    "\n",
    "r_ensemble = abs(cross_val_score(final_model, X, y, scoring='neg_log_loss'))\n",
    "results_ensemble.append(r_ensemble)\n",
    "print(f'Log Loss: {np.mean(results_ensemble).round(4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('xgb',\n",
       "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                            colsample_bylevel=1,\n",
       "                                            colsample_bynode=1,\n",
       "                                            colsample_bytree=0.11623733296601546,\n",
       "                                            enable_categorical=False,\n",
       "                                            eval_metric='logloss',\n",
       "                                            gamma=6.23346663072776e-05,\n",
       "                                            gpu_id=-1, importance_type=None,\n",
       "                                            interaction_constraints='',\n",
       "                                            learning_rate=0.0188649127427861,\n",
       "                                            max_delta_step=0, max_depth=...\n",
       "                                            subsample=0.8274515628887853,\n",
       "                                            tree_method='exact',\n",
       "                                            use_label_encoder=False,\n",
       "                                            validate_parameters=1, ...)),\n",
       "                             ('lgbm',\n",
       "                              LGBMClassifier(colsample_bytree=0.37621860984436856,\n",
       "                                             learning_rate=0.017847511069651788,\n",
       "                                             max_depth=27, min_child_samples=96,\n",
       "                                             n_estimators=422,\n",
       "                                             reg_alpha=3.7054057635827013e-09,\n",
       "                                             reg_lambda=4.861640891517785e-08,\n",
       "                                             subsample=0.26791725059736793,\n",
       "                                             verbose=-1))],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = final_model.predict_proba(test_final)\n",
    "\n",
    "res = pd.DataFrame(final_predictions, columns=['Status_C', 'Status_CL', 'Status_D'])\n",
    "res['id'] = test.id\n",
    "res = res[['id', 'Status_C', 'Status_CL', 'Status_D']]\n",
    "res.to_csv('submission_pca.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T05:22:04.248363Z",
     "iopub.status.busy": "2023-12-19T05:22:04.247914Z",
     "iopub.status.idle": "2023-12-19T05:22:04.308581Z",
     "shell.execute_reply": "2023-12-19T05:22:04.307270Z",
     "shell.execute_reply.started": "2023-12-19T05:22:04.248332Z"
    }
   },
   "outputs": [],
   "source": [
    "# catboost.fit(X, y)\n",
    "# pred = catboost.predict_proba(test_final)\n",
    "\n",
    "# res = pd.DataFrame(pred, columns=['Status_C', 'Status_CL', 'Status_D'])\n",
    "# res['id'] = test.id\n",
    "# res = res[['id', 'Status_C', 'Status_CL', 'Status_D']]\n",
    "# res.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7000181,
     "sourceId": 60893,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
